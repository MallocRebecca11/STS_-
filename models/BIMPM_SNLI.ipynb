{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BIMPM_SNLI.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"aD92bDhvMgl1"},"source":["import os\n","import copy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVsL2iYwMgl-"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","\n","from nltk import word_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMmIL7K9MgmA"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPNMCX2rMgmB"},"source":["from torchtext.legacy import data\n","from torchtext.legacy import datasets\n","from torchtext.vocab import GloVe,FastText"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GfIMSMXMgmC","executionInfo":{"status":"ok","timestamp":1638676900108,"user_tz":-480,"elapsed":1108,"user":{"displayName":"Jiarao Hong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15343010921258002984"}},"outputId":"4b0eb826-97f1-4789-a962-dbf8c8b54c23"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oBC7UOZdM1XK","executionInfo":{"status":"ok","timestamp":1638676904946,"user_tz":-480,"elapsed":4845,"user":{"displayName":"Jiarao Hong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15343010921258002984"}},"outputId":"a1006497-e890-4def-9ad0-ca0817a41988"},"source":["pip install tensorboardX"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n","\u001b[K     |████████████████████████████████| 124 kB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.4.1\n"]}]},{"cell_type":"code","metadata":{"id":"eDIu8czIMgmD"},"source":["from torch import nn, optim\n","from torch.autograd import Variable\n","from tensorboardX import SummaryWriter\n","from time import gmtime, strftime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JVh6kz6eMgmF"},"source":["SEED = 23\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVESv5oPMgmG"},"source":["params = {\n","'batch_size' : 128,\n","'char_emb_dim' : 20,\n","'char_emb_hs' : 50,\n","'data' :'snli',\n","'dropout' : 0.1,\n","'eps' : 10,\n","'gpu' : 0,\n","'hs' : 100,\n","'lr' : 1e-3,\n","'max_sent_len' : -1,\n","'num_perspective' : 20,\n","'print_freq' : 500,\n","'word_emb_dim' : 300,\n","'val_size' : 0.1,\n","'test_size' : 0.1,\n","'model_time' : 0\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKjzNlvpMgmI"},"source":["def SNLI_Data():\n","    TEXT = data.Field(batch_first=True, tokenize=word_tokenize, lower=True)\n","    LABEL = data.Field(sequential=False, unk_token=None)\n","    \n","    train, dev, test = datasets.SNLI.splits(TEXT, LABEL)\n","    TEXT.build_vocab(train, dev, test,vectors=GloVe(name='840B', dim=300), unk_init=torch.Tensor.zero_)\n","    LABEL.build_vocab(train)\n","    sort_key = lambda x: data.interleave_keys(len(x.q1), len(x.q2))\n","\n","    train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test),\\\n","                                    batch_sizes=[params['batch_size']] * 3,\n","                                   device=params['gpu'])\n","    iterator = {\n","        'train_iter' : train_iter,\n","        'dev_iter' : dev_iter,\n","        'test_iter' : test_iter\n","    }\n","    \n","    return iterator, TEXT, LABEL"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLMMHoUnMgmJ","executionInfo":{"status":"ok","timestamp":1638678153779,"user_tz":-480,"elapsed":1248352,"user":{"displayName":"Jiarao Hong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15343010921258002984"}},"outputId":"c5fa366b-900f-42cc-df43-6a3a1c0ee6fe"},"source":["iterator, TEXT, LABEL = SNLI_Data()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading snli_1.0.zip\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 94.6M/94.6M [01:17<00:00, 1.23MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["extracting\n"]},{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.840B.300d.zip: 2.18GB [06:51, 5.29MB/s]                            \n","100%|█████████▉| 2196016/2196017 [05:21<00:00, 6820.42it/s]\n","WARNING:torchtext.legacy.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n","WARNING:torchtext.legacy.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n","WARNING:torchtext.legacy.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"]}]},{"cell_type":"code","metadata":{"id":"xbffCIkZMgmK"},"source":["def Build_Char_Vocab(TEXT):\n","    max_word_len = 0\n","    for word in TEXT.vocab.itos:\n","        max_word_len = max(len(word), max_word_len)\n","    char_vocab = {'': 0}\n","    characterized_vectors = []\n","    for word in TEXT.vocab.itos:\n","        chars = []\n","        if word == '<unk>' or word == '<pad>':\n","            chars = (max_word_len*[0])\n","        else:\n","            for c in word:\n","                if c not in char_vocab:\n","                    char_vocab[c] = len(char_vocab)\n","                chars.append(char_vocab[c])\n","            \n","            chars.extend([0]*(max_word_len - len(word)))\n","        characterized_vectors.append(chars)\n","        \n","    return characterized_vectors, char_vocab, max_word_len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VpgNIJnFMgmL"},"source":["characterized_vectors, char_vocab, max_word_len = Build_Char_Vocab(TEXT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oltbByVMgmM"},"source":["def characterize(batch):\n","    batch = batch.data.cpu().numpy().astype(int).tolist()\n","    return [[characterized_vectors[w] for w in words] for words in batch]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXXvSdaYMgmM"},"source":["data =  {\n","    'TEXT' : TEXT,\n","    'LABEL' : LABEL,\n","    'iterator' : iterator,\n","    'characterized_vectors' : characterized_vectors,\n","    'char_vocab' : char_vocab,\n","    'max_word_len' : max_word_len,\n","    'word_vocab_size': len(TEXT.vocab),\n","    'char_vocab_size' : len(char_vocab),\n","    'class_size' : len(LABEL.vocab)\n","}    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JVf4DP2MgmN"},"source":["class BIMPM(nn.Module):\n","    def __init__(self, params, data):\n","        super(BIMPM, self).__init__()\n","\n","        self.params = params\n","        self.data = data\n","\n","        self.d = self.params['word_emb_dim'] + self.params['char_emb_hs']\n","        self.l = self.params['num_perspective']\n","\n","        \n","        self.char_emb = nn.Embedding(self.data['char_vocab_size'], self.params['char_emb_dim'], padding_idx=0)\n","        self.word_emb = nn.Embedding(self.data['word_vocab_size'], self.params['word_emb_dim'])\n","        self.word_emb.weight.data.copy_(data['TEXT'].vocab.vectors)\n","        self.word_emb.weight.requires_grad = False\n","        self.char_LSTM = nn.LSTM(input_size=self.params['char_emb_dim'],hidden_size=self.params['char_emb_hs'],batch_first=True,num_layers=1,bidirectional=False)\n","\n","        \n","      \n","        self.context_layer_LSTM = nn.LSTM(input_size=self.d, hidden_size=self.params['hs'],num_layers=1, \\\n","            bidirectional=True,batch_first=True)\n","        \n","        \n","        self.W1 = nn.Parameter(torch.rand(self.l, self.params['hs']))\n","        self.W2 = nn.Parameter(torch.rand(self.l, self.params['hs']))\n","\n","\n","        \n","        self.aggregation_layer_LSTM = nn.LSTM(input_size=self.l * 2,hidden_size=self.params['hs'],num_layers=1,\\\n","            bidirectional=True,batch_first=True)\n","\n","\n","        \n","        self.pred_fc1 = nn.Linear(self.params['hs'] * 4, self.params['hs'] * 2)\n","        self.pred_fc2 = nn.Linear(self.params['hs'] * 2, self.data['class_size'])\n","\n","        self.init_BIMPM()\n","        \n","    def Initialize_LSTM(self,LSTM, rev):\n","        \n","        nn.init.kaiming_normal_(LSTM.weight_ih_l0)\n","        nn.init.constant_(LSTM.bias_ih_l0, val=0)\n","        nn.init.orthogonal_(LSTM.weight_hh_l0)\n","        nn.init.constant_(LSTM.bias_hh_l0, val=0)\n","        \n","        if rev == 1:\n","            \n","            nn.init.kaiming_normal_(LSTM.weight_ih_l0_reverse)\n","            nn.init.constant_(LSTM.bias_ih_l0_reverse, val=0)\n","            nn.init.orthogonal_(LSTM.weight_hh_l0_reverse)\n","            nn.init.constant_(LSTM.bias_hh_l0_reverse, val=0)\n","\n","        return LSTM\n","\n","\n","    def init_BIMPM(self):\n","       \n","        nn.init.uniform_(self.char_emb.weight, -0.005, 0.005)\n","        self.char_emb.weight.data[0].fill_(0)\n","        nn.init.uniform_(self.word_emb.weight.data[0], -0.1, 0.1)\n","        self.char_LSTM = self.Initialize_LSTM(self.char_LSTM, 0)\n","\n","\n","        self.context_layer_LSTM = self.Initialize_LSTM(self.context_layer_LSTM, 1)\n","\n","\n","        nn.init.kaiming_normal_(self.W1)\n","        nn.init.kaiming_normal_(self.W2)\n","\n","\n","        self.aggregation_layer_LSTM = self.Initialize_LSTM(self.aggregation_layer_LSTM, 1)\n","\n","        \n","        nn.init.uniform_(self.pred_fc1.weight, -0.005, 0.005)\n","        nn.init.constant_(self.pred_fc1.bias, val=0)\n","        nn.init.uniform_(self.pred_fc2.weight, -0.005, 0.005)\n","        nn.init.constant_(self.pred_fc2.bias, val=0)\n","\n","    def dropout(self, v):\n","        return F.dropout(v, p=self.params['dropout'], training=self.training)\n","\n","    def forward(self, **kwargs):\n","        def match_fn(v1, v2, w):\n","            seq_len = v1.size(1)\n","            w = w.transpose(1, 0).unsqueeze(0).unsqueeze(0)\n","            v1 = w * torch.stack([v1] * self.l, dim=3)\n","            if len(v2.size()) == 3:\n","                v2 = w * torch.stack([v2] * self.l, dim=3)\n","            else:\n","                v2 = w * torch.stack([torch.stack([v2] * seq_len, dim=1)] * self.l, dim=3)\n","\n","            m = F.cosine_similarity(v1, v2, dim=2)\n","\n","            return m\n","\n","\n","        p = self.word_emb(kwargs['p'])\n","        h = self.word_emb(kwargs['h'])\n","\n","        seq_len_p = kwargs['char_p'].size(1)\n","        seq_len_h = kwargs['char_h'].size(1)\n","\n","        char_p = kwargs['char_p'].view(-1, data['max_word_len'])\n","        char_h = kwargs['char_h'].view(-1, data['max_word_len'])\n","\n","        _, (char_p, _) = self.char_LSTM(self.char_emb(char_p))\n","        _, (char_h, _) = self.char_LSTM(self.char_emb(char_h))\n","\n","        char_p = char_p.view(-1, seq_len_p, self.params['char_emb_hs'])\n","        char_h = char_h.view(-1, seq_len_h, self.params['char_emb_hs'])\n","\n","        p = torch.cat([p, char_p], dim=-1)\n","        h = torch.cat([h, char_h], dim=-1)\n","\n","        p = self.dropout(p)\n","        h = self.dropout(h)\n","\n","        con_p, _ = self.context_layer_LSTM(p)\n","        con_h, _ = self.context_layer_LSTM(h)\n","\n","        con_p = self.dropout(con_p)\n","        con_h = self.dropout(con_h)\n","\n","        con_p_fw, con_p_bw = torch.split(con_p, self.params['hs'], dim=-1)\n","        con_h_fw, con_h_bw = torch.split(con_h, self.params['hs'], dim=-1)\n","\n","\n","        mv_p_full_fw = match_fn(con_p_fw, con_h_fw[:, -1, :], self.W1)\n","        mv_p_full_bw = match_fn(con_p_bw, con_h_bw[:, 0, :], self.W2)\n","        mv_h_full_fw = match_fn(con_h_fw, con_p_fw[:, -1, :], self.W1)\n","        mv_h_full_bw = match_fn(con_h_bw, con_p_bw[:, 0, :], self.W2)\n","        \n","        mv_p = torch.cat(\n","            [mv_p_full_fw,mv_p_full_bw], dim=2)\n","        mv_h = torch.cat(\n","            [mv_h_full_fw,mv_h_full_bw], dim=2)\n","\n","        mv_p = self.dropout(mv_p)\n","        mv_h = self.dropout(mv_h)\n","\n","        _, (agg_p_last, _) = self.aggregation_layer_LSTM(mv_p)\n","        _, (agg_h_last, _) = self.aggregation_layer_LSTM(mv_h)\n","\n","        x = torch.cat(\n","            [agg_p_last.permute(1, 0, 2).contiguous().view(-1, self.params['hs'] * 2),\n","             agg_h_last.permute(1, 0, 2).contiguous().view(-1, self.params['hs'] * 2)], dim=1)\n","        x = self.dropout(x)\n","\n","        x = F.tanh(self.pred_fc1(x))\n","        x = self.dropout(x)\n","        x = self.pred_fc2(x)\n","\n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z4MjsxMNMgmO"},"source":["def test(model, params, data, mode=1):\n","  \n","    if mode == 0:\n","        iterator = iter(data['iterator']['dev_iter'])\n","    else:\n","        iterator = iter(data['iterator']['test_iter'])\n","\n","    criterion = nn.CrossEntropyLoss()\n","    model.eval()\n","    acc, loss, size = 0, 0, 0\n","\n","    \n","    for batch in iterator:\n","        s1, s2 = 'premise', 'hypothesis'\n","        \n","        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n","        \n","        if params['gpu'] > -1:\n","            s1 = s1.cuda(params['gpu'])\n","            s2 = s2.cuda(params['gpu'])\n","\n","        kwargs = {'p': s1, 'h': s2}\n","\n","        char_p = Variable(torch.LongTensor(characterize(s1)))\n","        char_h = Variable(torch.LongTensor(characterize(s2)))\n","\n","        if params['gpu'] > -1:\n","            char_p = char_p.cuda(params['gpu'])\n","            char_h = char_h.cuda(params['gpu'])\n","\n","        kwargs['char_p'] = char_p\n","        kwargs['char_h'] = char_h\n","\n","        pred = model(**kwargs)\n","\n","        batch_loss = criterion(pred, batch.label.cuda(params['gpu']))\n","        loss += batch_loss.data.item()\n","\n","        _, pred = pred.max(dim=1)\n","        acc += (pred == batch.label.cuda(params['gpu'])).sum().float()\n","        size += len(pred)\n","\n","    acc /= size\n","    acc = acc.cpu().data.item()\n","    return loss, acc\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NCXbKfHfqeMX","executionInfo":{"status":"ok","timestamp":1638678188622,"user_tz":-480,"elapsed":34342,"user":{"displayName":"Jiarao Hong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15343010921258002984"}},"outputId":"94112e2c-8a89-4b08-918a-8e8280c7db1a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"80i0AkGWMgmO"},"source":["def train(params, data):\n","    model = BIMPM(params, data)\n","\n","    if params['gpu'] > -1:\n","        model.cuda(params['gpu'])\n","\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optimizer = optim.AdamW(parameters, lr=params['lr'])\n","    criterion = nn.CrossEntropyLoss()\n","\n","\n","    model.train()\n","    loss, last_epoch = 0, -1\n","    max_dev_acc, max_test_acc = 0, 0\n","\n","    iterator = data['iterator']['train_iter']\n","    train_loss = []\n","    val_loss = []\n","    epochs = params['eps']\n","    \n","    while epochs != 0:\n","        epochs -= 1\n","        epoch_loss = 0\n","#         cnt_batches = 0\n","        print(\"Epoch: %s\" %(params['eps']- epochs))\n","        for i, batch in enumerate(iterator):\n","#             cnt_batches += 1\n","            s1, s2 = 'premise', 'hypothesis'\n","\n","\n","            s1, s2 = getattr(batch, s1), getattr(batch, s2)\n","\n","            if params['max_sent_len'] >= 0:\n","                if s1.size()[1] > params['max_sent_len']:\n","                    s1 = s1[:, :params['max_sent_len']]\n","                if s2.size()[1] > params['max_sent_len']:\n","                    s2 = s2[:, :params['max_sent_len']]\n","\n","            if params['gpu'] > -1:\n","                s1 = s1.cuda(params['gpu'])\n","                s2 = s2.cuda(params['gpu'])\n","\n","            kwargs = {'p': s1, 'h': s2}\n","\n","            char_p = Variable(torch.LongTensor(characterize(s1)))\n","            char_h = Variable(torch.LongTensor(characterize(s2)))\n","\n","            if params['gpu'] > -1:\n","                char_p = char_p.cuda(params['gpu'])\n","                char_h = char_h.cuda(params['gpu'])\n","\n","            kwargs['char_p'] = char_p\n","            kwargs['char_h'] = char_h\n","\n","            pred = model(**kwargs)\n","\n","            pred = pred.cuda(params['gpu'])\n","\n","            optimizer.zero_grad()\n","            batch_loss = criterion(pred, batch.label.cuda(params['gpu']))\n","            loss += batch_loss.data.item()\n","            epoch_loss += batch_loss.data.item()\n","            batch_loss.backward()\n","            optimizer.step()\n","\n","            if (i + 1) % params['print_freq'] == 0:\n","                dev_loss, dev_acc = test(model, params, data, mode=0)\n","                test_loss, test_acc = test(model, params, data)\n","                c = (i + 1) // params['print_freq']\n","\n","                print('train loss: ',loss, 'dev loss: ',dev_loss,'test loss: ', test_loss, 'dev acc: ', dev_acc , 'test acc: ', test_acc)\n","\n","                if dev_acc > max_dev_acc:\n","                    max_dev_acc = dev_acc\n","                    max_test_acc = test_acc\n","                    if not os.path.exists('saved_models'):\n","                        os.makedirs('saved_models')\n","                    torch.save(model.state_dict(), f'/content/drive/My Drive/BIBPM_snli.pt')\n","                loss = 0\n","                model.train()\n","                \n","        train_loss.append(epoch_loss)\n","        \n","        dev_loss, dev_acc = test(model, params, data, mode=0)\n","        val_loss.append(dev_loss)\n","        model.train()\n","\n","    print('max dev acc:', max_dev_acc, 'max test acc: ', max_test_acc)\n","    return train_loss, val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3T_i41Ki8Cn","executionInfo":{"status":"ok","timestamp":1638678188628,"user_tz":-480,"elapsed":45,"user":{"displayName":"Jiarao Hong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15343010921258002984"}},"outputId":"bf45b102-178c-43d1-c48f-7b4a573c82e0"},"source":["torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"cYtWO6qfk4So"},"source":["os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"lJYUXIBIMgmQ","scrolled":true,"outputId":"c8e31c3c-6a88-4149-d961-264398afbae4"},"source":["params['model_time'] = strftime('%H:%M:%S', gmtime())\n","print('training start!')\n","\n","train_loss, val_loss = train(params, data)\n","\n","print('training finished!')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["training start!\n","Epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"]},{"output_type":"stream","name":"stdout","text":["train loss:  436.9334970712662 dev loss:  65.36059820652008 test loss:  65.04123485088348 dev acc:  0.6152204871177673 test acc:  0.6164494752883911\n","train loss:  368.43088537454605 dev loss:  60.96387821435928 test loss:  60.3430569767952 dev acc:  0.6473277807235718 test acc:  0.6560463905334473\n","train loss:  340.7125427722931 dev loss:  55.88416013121605 test loss:  55.64218792319298 dev acc:  0.6816703677177429 test acc:  0.6848533749580383\n","train loss:  330.08575227856636 dev loss:  52.835997968912125 test loss:  53.12505793571472 dev acc:  0.7086974382400513 test acc:  0.7112174034118652\n","train loss:  315.78062266111374 dev loss:  51.00493040680885 test loss:  50.9591184258461 dev acc:  0.7189595699310303 test acc:  0.7192589044570923\n","train loss:  304.8054034113884 dev loss:  49.640381544828415 test loss:  50.05420270562172 dev acc:  0.7310506105422974 test acc:  0.7274022698402405\n","train loss:  299.10447415709496 dev loss:  51.36740177869797 test loss:  51.60782352089882 dev acc:  0.712660014629364 test acc:  0.7111155986785889\n","train loss:  294.6215877532959 dev loss:  49.26726081967354 test loss:  49.455827474594116 dev acc:  0.7295265197753906 test acc:  0.7288273572921753\n","Epoch: 2\n","train loss:  448.35734063386917 dev loss:  49.789204359054565 test loss:  49.85143303871155 dev acc:  0.7248526811599731 test acc:  0.7306596040725708\n","train loss:  274.28741654753685 dev loss:  47.57335340976715 test loss:  48.09863033890724 dev acc:  0.7412111163139343 test acc:  0.7417548894882202\n","train loss:  273.4115350842476 dev loss:  46.91841250658035 test loss:  47.177770018577576 dev acc:  0.7430400252342224 test acc:  0.7471498250961304\n","train loss:  273.4144224822521 dev loss:  47.89604955911636 test loss:  48.39107409119606 dev acc:  0.7449705600738525 test acc:  0.7435871362686157\n","train loss:  271.02291864156723 dev loss:  45.98386025428772 test loss:  46.48445871472359 dev acc:  0.7501524090766907 test acc:  0.7461318969726562\n","train loss:  267.17172649502754 dev loss:  46.789357364177704 test loss:  46.79206299781799 dev acc:  0.7460882067680359 test acc:  0.7486766576766968\n","train loss:  265.82102274894714 dev loss:  43.38713410496712 test loss:  43.62713572382927 dev acc:  0.7665108442306519 test acc:  0.7663884162902832\n","train loss:  264.8212677538395 dev loss:  43.02822807431221 test loss:  43.08959540724754 dev acc:  0.769355833530426 test acc:  0.7693403363227844\n","Epoch: 3\n","train loss:  413.8193030357361 dev loss:  45.450514793395996 test loss:  44.61455190181732 dev acc:  0.760617733001709 test acc:  0.7592629790306091\n","train loss:  255.3821566402912 dev loss:  43.5662425160408 test loss:  43.8751845061779 dev acc:  0.7704734802246094 test acc:  0.768017053604126\n","train loss:  251.7712543606758 dev loss:  44.957331359386444 test loss:  45.09744518995285 dev acc:  0.7613289952278137 test acc:  0.7616041898727417\n"]}]},{"cell_type":"code","metadata":{"id":"Pt3ObZUPMgmR"},"source":["from sklearn.metrics import fbeta_score, precision_recall_fscore_support, f1_score,confusion_matrix,plot_confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aesXic3uMgmR"},"source":["def plot_loss(train_loss, val_loss):\n","    \n","    X = [i for i in range(1,params['eps']+1)]\n","    \n","    plt.plot(X,train_loss)\n","    plt.ylabel('train-loss')\n","    plt.xlabel('epoch')\n","    plt.show()\n","    \n","    plt.plot(X,val_loss)\n","    plt.ylabel('val-loss')\n","    plt.xlabel('epoch')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2C3kIShKMgmS"},"source":["plot_loss(train_loss, val_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4SdLNaGMgmS"},"source":["def load_model(params, data):\n","    model = BIMPM(params, data)\n","    model.load_state_dict(torch.load('/content/drive/My Drive/models/BIBPM_snli.pt'))\n","\n","    if params['gpu'] > -1:\n","        model.cuda(params['gpu'])\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FC0WQzqdMgmS"},"source":["def test1(model, params, data):\n","\n","    iterator = iter(data['iterator']['test_iter'])\n","\n","    criterion = nn.CrossEntropyLoss()\n","    model.eval()\n","    acc, loss, size = 0, 0, 0\n","\n","    actual = list()\n","    preds = list()\n","    \n","    for batch in iterator:\n","        s1, s2 = 'premise', 'hypothesis'\n","        \n","        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n","        \n","        if params['gpu'] > -1:\n","            s1 = s1.cuda(params['gpu'])\n","            s2 = s2.cuda(params['gpu'])\n","\n","        kwargs = {'p': s1, 'h': s2}\n","\n","        char_p = Variable(torch.LongTensor(characterize(s1)))\n","        char_h = Variable(torch.LongTensor(characterize(s2)))\n","\n","        if params['gpu'] > -1:\n","            char_p = char_p.cuda(params['gpu'])\n","            char_h = char_h.cuda(params['gpu'])\n","\n","        kwargs['char_p'] = char_p\n","        kwargs['char_h'] = char_h\n","\n","        pred = model(**kwargs)\n","        \n","\n","        actual.extend(batch.label)\n","        preds.extend(pred.detach().cpu())\n","\n","        batch_loss = criterion(pred, batch.label.cuda(params['gpu']))\n","        loss += batch_loss.data.item()\n","\n","        _, pred = pred.max(dim=1)\n","        acc += (pred == batch.label.cuda(params['gpu'])).sum().float()\n","        size += len(pred)\n","\n","    acc /= size\n","    acc = acc.cpu().data.item()\n","    \n","\n","    return loss, acc, preds, actual \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBcFN0dtMgmT"},"source":["model = load_model(params, data)\n","loss , acc , preds, actual = test1(model, params, data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lAPs7a3MgmU"},"source":["def decode_sentences(sen):\n","    sentence = \"\"\n","    for i in sen:\n","        if(TEXT.vocab.itos[i] != '<pad>'):\n","            sentence += TEXT.vocab.itos[i] + \" \"\n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Hmx6q1yMgmU"},"source":["iterator = iter(data['iterator']['test_iter'])\n","sentences1 = []\n","sentences2 = []\n","\n","for batch in iterator:\n","        s1, s2 = 'premise', 'hypothesis'\n","        \n","        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n","        \n","        for i in range(0,len(s1)):\n","            sentences1.append(decode_sentences(s1[i]))\n","            sentences2.append(decode_sentences(s2[i])) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhUcV92RMgmV"},"source":["temp = preds\n","predicted_classes = []\n","for ten in temp:\n","    predicted_classes.append(torch.argmax(ten))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smRIIY--MgmV"},"source":["for i in range(0,len(actual)):\n","    if(actual[i] != predicted_classes[i]):\n","        print(\"premise: \",sentences1[i],\"hypothesis: \",sentences2[i],\"actual_label: \" ,actual[i].tolist() ,\"predicted_label: \",predicted_classes[i].tolist())\n","        print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cp8aY8xRMgmW"},"source":["precision, recall, f1_measure, _ = precision_recall_fscore_support(actual, predicted_classes, average='macro')\n","\n","print(\"Precision:\",precision)\n","print(\"Recall:\",recall)\n","print(\"f1_score:\",f1_measure)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-81D_rWTMgmW"},"source":["conf = confusion_matrix(actual, predicted_classes)\n","print(\"Confusion Matrix:\\n\",conf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3eXQ0-ZMgmW"},"source":["sns.heatmap(conf, annot=True); #annot=True to annotate cells"],"execution_count":null,"outputs":[]}]}